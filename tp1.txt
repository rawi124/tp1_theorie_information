EXERCICE 1
************************************************************************************************************************************************
L'information mutuelle mesure la quantité d'information apportée en moyenne par une réalisation de X sur les probabilités de réalisation de Y.
En considérant qu'une distribution de probabilité représente notre connaissance sur un phénomène aléatoire, on mesure l'absence d'information par 
l'entropie de cette distribution. En ces termes, l'information mutuelle s'exprime par:

{\displaystyle I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X)+H(Y)-H(X,Y).}{\displaystyle I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X)+H(Y)-H(X,Y).}
où H(X) et H(Y) sont des entropies, H(X|Y) et H(Y|X) sont des entropies conditionnelles, et H(Y, X) est l'entropie conjointe entre X et Y.

Ainsi on voit que {\displaystyle I(X;Y)=0}{\displaystyle I(X;Y)=0} ssi le nombre de bits nécessaires pour coder une réalisation du couple est égal 
à la somme du nombre de bits pour coder une réalisation de X et du nombre de bits pour coder une réalisation de Y
************************************************************************************************************************************************